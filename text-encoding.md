ASCII is 7 bits, meaning it can be a number between [0, 127] (2^7 = 128)

A byte is usually 8 bits, so in ASCII the first bit is simply 0. This is what utf-8 exploits. UTF-8 can encode 2^3*2^6*2^6*2^6
as it has 3 bits available in the co

Text Encodings:

| Letter |               UTF-8 |          ISO-8859-1 |
|:-------|--------------------:|--------------------:|
| `æ`    | `11000011 10100110` |          `11100110` |
| `ø`    | `11000011 10111000` |          `11111000` |
| `å`    | `11000011 10100101` |          `11100101` |
| `Æ`    | `11000011 10000110` |          `11000110` |
| `Ø`    | `11000011 10011000` |          `11011000` |
| `Å`    | `11000011 10000101` |          `11000101` |
| `Ã¦`   |                     | `11000011 10100110` |
| `Ã¸`   |                     | `11000011 10111000` |
| `Ã¥`   |                     | `11000011 10100101` |
| `Ã`    |                     | `11000011 10000110` |
| `Ã`    |                     | `11000011 10011000` |
| `Ã `   |                     | `11000011 10000101` |
